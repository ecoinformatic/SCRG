# Model-Selection Workflow

The sections below document the process used to prepare data for model selection for the SCRG meta-analysis. This includes the following scripts:
* Wrangling and Cleaning Data (`wranglingCleaning.R`)
* Standardizing Predictors (`standardize.R`)
* Pruning & Model Selection (`pruning.R`)

Additionally, the following script is the build-up/pair-down model selection code used to identify the most parsimonious set of predictors for the statewide model:
* Build-Up, Pair-Down Model Selection Script (`BUPD`)

The following packages are required:
* `sf`
* `dplyr`
* `tidyr`

The output from this script (e.g., beta values) can be used in downstream workflows, such as meta-analyses.

## Wrangling and Cleaning Data (`wranglingCleaning.R`)
The following script imports datasets generated by the following LSSMs:
* [Choctawhatchee Bay Living Shoreline Model (VIMS)](https://www.arcgis.com/home/webmap/viewer.html?webmap=9b2ef272b1af429fa38e67134ac49da3)
* [Pensacola Bay Living Shoreline Model (VIMS)](https://www.arcgis.com/apps/instant/interactivelegend/index.html?appid=e9eee63681694840a5a7d8ee41f19eb6)
* [Living Shoreline Suitability Model for Tampa Bay (VIMS)](https://docs.google.com/spreadsheets/d/1jZmUNlY68Eb_SUPPAjriNE0UJUHFtkh_xXRLeNGBI_g/edit?gid=0#gid=0)
* [Shoreline Restoration Suitability Model for North Indian River and Mosquito Lagoon](https://ucfonline.maps.arcgis.com/apps/MapSeries/index.html?appid=45caa29e80e6441c8bf6f75c542860af)

In this script, certain types of columns are removed to retain only relevant predictor and response variables. Columns containing non-predictor metadata (e.g., those associated with the date of study or link to materials) and columns containing geometry data (e.g., those needed for GIS shapefiles but not for model selection) are removed. Additionally, spelling checks are perfomed to ensure column naming is consistent across datasets (e.g., replacing 'widebeach' with 'WideBeach'), and that values within columns are correctly spelled and formated (e.g., replacing "YEs" with "Yes").

```{R, echo=FALSE}
rm(list=ls())
library(sf)
library(dplyr)

# Choctawatchee data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
choc <- st_transform(st_read("Data/choctawatchee_bay/choctawatchee_bay_lssm_POINTS_0.001deg.shp"), crs = 6346) # Reponse: SMMv5Class

# Pensacola data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
pens <- st_transform(st_read("Data/santa_rosa_bay/Santa_Rosa_Bay_Living_Shoreline_POINTS_0.001deg.shp"), crs = 6346) # Reponse: SMMv5Class

# Tampa data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
tampa <- st_transform(st_read("Data/tampa_bay/Tampa_Bay_Living_Shoreline_Suitability_Model_Results_POINTS_0.001deg.shp"), crs = 6346) # Response: 

# IRL data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
IRL <- st_transform(st_read("Data/indian_river_lagoon/UCF_livingshorelinemodels_MosquitoNorthIRL_111m.shp"), crs = 6346)

####################################
# RESPONSE VARIABLES
####################################
# choc$SMMv5Class, pens$SMMv5Class, tampa$BMPallSMM # Reponse variables for VIMS
# IRL$Priority $ Response variable for IRL study

####################################
# PREDICTOR DATA
####################################
# Columns that are geodata/metadata and can be removed across studies
choc <- st_drop_geometry(choc)
pens <- st_drop_geometry(pens)
tampa <- st_drop_geometry(tampa)
IRL <- st_drop_geometry(IRL)
drop <- c("OBJECTID", "ID", "geometry", "feature_x", "feature_y", "nearest_x", "nearest_y", "shape__len", "Shape__Len", "distance", "distance_2", "n", "x", "y", "X", "Y")
choc <- choc[, !(colnames(choc) %in% drop)]
pens <- pens[, !(colnames(pens) %in% drop)]
tampa <- tampa[, !(colnames(tampa) %in% drop)]
IRL <- IRL[, !(colnames(IRL) %in% drop)]
# Columns that can be removed from individual studies 
# Remove metadata columns and other shapefile stuff
choc <- choc[, !(colnames(choc) %in% c("DefDate", "Needs_QC", "bmpCountv5", "SMMv5Def"))] # DefDate is just a date of data entry
pens <- pens[, !(colnames(pens) %in% c("Additional", "Permitting"))] # "Additional" and "Permitting" are links in the pens data
tampa <- tampa[, !(colnames(tampa) %in% c("Source"))]
IRL <- IRL[, !(colnames(IRL) %in% c("Comments"))]

# Rename columns to be consisent
## Columns that should be the same:
### choc$MxQExpCode, pens$exposure, tampa$Exposure
### choc$Beach, pens$beach, tampa$Beach
### choc$WideBeach, pens$widebeach, tampa$WideBeach
choc <- choc %>% rename(
  Exposure = MxQExpCode,
  Beach = Beach,
  WideBeach = WideBeach,
  Response = SMMv5Class
)
pens <- pens %>% rename(
  Exposure = exposure,
  Beach = beach,
  WideBeach = widebeach,
  Response = SMMv5Class
)
tampa <- tampa %>% rename(
  Exposure = Exposure,
  Beach = Beach,
  WideBeach = WideBeach,
  Response = BMPallSMM
)
IRL$Priority <- as.character(IRL$Priority)
IRL <- IRL %>% rename(
  Response = Priority
)
####################################
# CONVERT RESPONSE TO CONTINUOUS
####################################
library(dplyr)

########## IRL #########
IRL <- IRL %>% 
  mutate(Response = as.numeric(case_when(
    Response %in% c("0", "1") ~ "1",
    Response %in% c("2", "3") ~ "2",
    Response %in% c("4", "5") ~ "3",
    TRUE                       ~ Response  # keeps other values unchanged
  )))

########## choc #########
choc <- choc %>%
  mutate(Response = as.numeric(case_when(
    Response %in% c("Maintain Beach or Offshore Breakwater with Beach Nourishment",
                    "Non-Structural Living Shoreline",
                    "Plant Marsh with Sill", "Existing Marsh Sill", "Existing Breakwater") ~ "3",
    Response %in% c("Ecological Conflicts. Seek regulatory advice.",
                    "Highly Modified Area. Seek expert advice.",
                    "Land Use Management",
                    "No Action Needed",
                    "Special Geomorphic Feature. Seek expert advice.") ~ "2",
    Response %in% c("Groin Field with Beach Nourishment",
                    "Revetment",
                    "Revetment/Bulkhead Toe Revetment") ~ "1",
    TRUE ~ "1"  # NAs to 1
  )))

########## pens #########
pens <- pens %>%
  mutate(Response = as.numeric(case_when(
    Response %in% c("Maintain Beach or Offshore Breakwater with Beach Nourishment",
                    "Non-Structural Living Shoreline",
                    "Plant Marsh with Sill", "Existing Breakwater") ~ "3",
    Response %in% c("Ecological Conflicts. Seek regulatory advice.",
                    "Highly Modified Area. Seek expert advice.",
                    "Land Use Management",
                    "No Action Needed",
                    "Special Geomorphic Feature. Seek expert advice.") ~ "2",
    Response %in% c("Groin Field with Beach Nourishment",
                    "Revetment",
                    "Revetment/Bulkhead Toe Revetment Replacement") ~ "1",
    TRUE ~ "1"  # NAs to 1
  )))

########## tampa #########
# tricky case due to <br> and other descriptions in columns
## see: https://ocean.floridamarine.org/arcgis/rest/services/Projects_FWC/livingShorelineTB/MapServer/0
tampa <- tampa %>%
  mutate(Response = case_when(
    grepl("Maintain Beach OR Offshore Breakwaters with Beach Nourishment|Non-Structural Living Shoreline|Plant Marsh with Sill|Maintain/Enhance/Create Marsh|Restore Riparian Buffer|Option 1|Option 2 or 5", Response) ~ 3,
    grepl("Ecological Conflicts|Highly Modified Area|Land Use Management|Special Geomorphic|No Action Needed", Response) ~ 2,
    grepl("Revetment|Groin Field with Beach Nourishment|Option B3 or B4|Option B8 or B9|Option R7 or R8|Option B7|Option R3 or R4|Option 6", Response) ~ 1,
    TRUE ~ NA_real_  # Ensuring this branch also explicitly returns a numeric type
  ))

####################################
# COMBINE DATA
####################################
# Add a 'study' column
choc$study <- 'choc'
pens$study <- 'pens'
tampa$study <- 'tampa'
IRL$study <- 'IRL'
# Combine Data
state <- dplyr::bind_rows(choc, pens, tampa, IRL) 
pred <- state %>% 
  select(-"Response") # Remove response variables
####################################
# SPELL CHECK
####################################
library(hunspell)
library(dplyr)
library(stringr)

# Review misspelled/suggestions
words_to_check <- unique(unlist(pred))
misspelled_info <- hunspell_check(words_to_check)
misspelled_words <- words_to_check[!misspelled_info]
suggestions <- hunspell_suggest(misspelled_words)
print(data.frame(misspelled = misspelled_words, suggestions = I(suggestions)))

# Spelling corrections (words needs to be chosen manually)
corrections <- data.frame(
  incorrect = c("YEs", "RIprap", "riprap", "Permament", "Permanenet", "Bulkead", "Bulkhea", "BUlkhead"),
  correct = c("Yes", "Riprap", "Riprap", "Permanent", "Permanent", "Bulkhead", "Bulkhead", "Bulkhead")
)
pred <- pred %>%
  mutate(across(where(is.character), ~{
    column <- .
    for (i in seq_along(corrections$incorrect)) {
      # Use regex to match case-insensitively
      pattern <- str_c("(?i)\\b", corrections$incorrect[i], "\\b")
      column <- str_replace_all(column, regex(pattern), corrections$correct[i])
    }
    column
  }))
```

## A.1/A.2: Standardizing Predictors (`standardize.R`)

### Numerical Variables
To ensure models run without errors, missing values (NA) must be replaced. For numerical variables, we can replace the missing values with the statewide mean (calculated as the mean across all regional datasets). They can then be standardized across studies for consistency.

```{R, echo=FALSE}
library(tidyr)

#########################
# NUMERICAL VARS
#########################
# List numerical vars
numerical_vars <- c("angle", "IT_Width", "Hab_W1", 
                    "Hab_W2", "Hab_W3", "Hab_W4", "Slope", "X3_m_depth", "X5_m_depth", "Slope_4",
                     "X10th", "X20th", "X30th", "X40th", "X50th", "X60th", "X70th", "X80th",
                     "X90th", "X99th", "MANGROVE")
pred <- pred %>% 
    mutate(across(all_of(numerical_vars), as.numeric)) # convert them to numeric if not already

# Replace NAs with means
pred <- pred %>%
  mutate(across(all_of(numerical_vars), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Standardize numeric vars
pred <- pred %>%
  mutate(across(all_of(numerical_vars), ~ (.-mean(., na.rm = TRUE))/sd(., na.rm = TRUE)))

# Checks
## make sure sd is close to 1 and mean is close to 0
summary(pred[numerical_vars]) # summary stats
sapply(pred[numerical_vars], sd, na.rm = TRUE) # stdev

#########################
# CATEGORICAL VARS
#########################
# List cat vars
categorical_vars <- c("bnk_height", "Beach", "WideBeach", "Exposure", "bathymetry", 
                      "roads", "PermStruc", "PublicRamp", "RiparianLU", "canal", 
                      "SandSpit", "Structure", "offshorest", "SAV", "marsh_all", 
                      "tribs", "defended", "rd_pstruc", "lowBnkStrc", "ShlType", 
                      "Fetch_", "selectThis", "StrucList", "forestshl", 
                      "City", "Point_Type", "Edge_Type", "Hard_Mater", "Adj_LU", 
                      "Erosion", "Erosion_2", "Owner", "Adj_H1", "Adj_H2", "Adj_H3", "Adj_H4", "V_Type1", 
                      "V_Type2", "V_Type3", "V_Type4", "Rest_Opp", "X0yster_Pre", "Seagrass_P",
                      "Hardened_1", "WTLD_VEG_3") 
                      # note that study column is excluded here for easier processing later
pred <- pred %>%
    mutate(across(all_of(categorical_vars), as.factor))

##### DUMMY VARIABLES #####
# Replace NA with "Missing" for dummy vars
pred <- pred %>%
  mutate(across(all_of(categorical_vars), ~ factor(ifelse(is.na(.), "Missing", .), levels = unique(c(.,"Missing")))))

# Make dummy vars
for (var in categorical_vars) {
  dummies <- model.matrix(~ . - 1, data = pred[var])  # suggested to avoid itercept
  colnames(dummies) <- paste(var, levels(pred[[var]]), sep = "_")
  pred <- cbind(pred, as.data.frame(dummies))
}

# Remove OG categorical columns
pred <- pred %>%
  select(-all_of(setdiff(categorical_vars, "study")))

# Remove "Length" columns
pred <- pred %>%
  select(-contains(c("Length", "Lgth")))

# Check data
str(pred)
```

## A.3/A.4: Pruning & Model Selection (`pruning.R`)
The following script is used to prepare predictor and response variables for the model selection workflow contained in `BUPD.R`. The script generates the following outputs, though additional outputs can be added as necessary:
* `"_final_model.rds"`: Contains model summary and slopes for predictors
* `"_final_form.rds"`: Contains final model formula with best AIC
* `"_odds_ratios.rds"`: Contains odds ratios for predictors

```{R, echo=FALSE}
#############################################
# GET RESPONSE AND STANDARDIZED PREDICTORS
#############################################
library(dplyr)
resp <- as.data.frame(cbind(state$Response, state$study))
colnames(resp) <- c("Response", "study")

resp_choc <- resp %>% filter(study == "choc")
resp_pens <- resp %>% filter(study == "pens")
resp_tampa <- resp %>% filter(study == "tampa")
resp_IRL <- resp %>% filter(study == "IRL")

pred_choc <- pred %>% filter(study == "choc")
pred_pens <- pred %>% filter(study == "pens")
pred_tampa <- pred %>% filter(study == "tampa")
pred_IRL <- pred %>% filter(study == "IRL")

##### CHOSE STUDY HERE #####
# combine response and pred
data <- cbind(resp_IRL, pred_IRL) # choc example

# Specify a short name of the model
name <- "non_parallel_test"
############################

# Grab categorical variables (dummyvars has the separated out names/dummy variables)
dummyvars <- colnames(pred)[grepl("_", colnames(pred))]

# List predictors (AKA column names of known predictors)
predictors <- c(numerical_vars, dummyvars)

# Define the response variable
response_var <- "Response" 

# NEW! 
# resp <- data.frame(Response = state$Response)
resp <- data.frame(Response = factor(state$Response, ordered = TRUE))


study <- data.frame(study = state$study)
input <- cbind(resp, pred)
input$SMMv5Def <- NULL
input$study <- as.factor(input$study)

# # Run build-up/pair-down R scripts
start_time <- Sys.time()
source("ecoinfoscrg/R/MetaAnalysis/BUPD_nonparallel.R")
end_time <- Sys.time()
```

## Build-Up, Pair-Down Model Selection Script (`BUPD`)
The following is the build-up, pair-down script used to perform model selection:

### Non-Parallel (Single-Core) Version
```{R, echo=FALSE}
############################################
# INITIALIZE
############################################
# library(nnet)
# library(lme4)
library(MASS)
# library(ordinal)

# Initial empty model
# Note that `best_formula` starts with `initial_formula` as baseline 
# initial_formula <- as.formula(paste(response_var, "~ 1 + (1|study)"))
initial_formula <- as.formula(paste(response_var, "~ 1"))
best_formula <- initial_formula

# best_model <- multinom(best_formula, data = data, MaxNWts = 5000) #nnet
# best_model <- lmer(best_formula, data = input) # lme4
# best_model <- polr(best_formula, data = input, method = "probit") # MASS
best_model <- polr(best_formula, data = input, Hess=FALSE, method="probit")
# control_settings <- clmm.control(maxIter = 2000)  # clmm stuff
# best_model <- clmm(best_formula, data = input, control = control_settings, link = "probit") # ordinal
# best_model <- glmer(best_formula, data = input, family=binomial(link="probit"))
# best_model <- glm(best_formula, data = input, family=binomial(link="probit"))
# glmer(Response ~ (1 | study) + Beach_1, data = input, family=binomial(link="probit"))

best_aic <- AIC(best_model)
name_prefix <- gsub(" ", "_", name) # add underscores
############################################
# BUILD-UP PHASE
############################################
# Function to fit and evaluate models (similar to Chris' only with a multinomial logistic regression for this specific dataset)
fit_model <- function(formula, data) {
  # model <- lmer(formula, data = data)
  # model <- clmm(formula, data = data, link = "probit")
  # model <- polr(formula, data = data, method = "probit")
  tryCatch({ # NEW LINE
    model <- polr(formula, data = data, Hess=FALSE, method="probit")
    aic <- AIC(model) # NEW LINE
    return(list(model = model, aic = aic, error = NULL))
  }, error = function(e) { # NEW LINE
    return(list(model = NULL, aic = Inf, error = e$message))
  })
}

# Similar to Chris' only with a multinomial logistic regression (which can be switched out)
for (i in 1:length(predictors)) {
    current_predictors <- all.vars(best_formula)[-1]
    remaining_predictors <- setdiff(predictors, current_predictors) 
    candidate_models <- list()  # for storing model and their AIC
    # Build models iteratively based on current best model
    for (predictor in remaining_predictors) {
        new_formula <- update(best_formula, paste(". ~ . +", predictor))
        formula_str <- paste(deparse(new_formula, width.cutoff = 500), collapse = "") # converts model formulat to string. Note that the width.cutoff and collapse are EXTREMELY important or deparse will split your string into two lines by default
        fit <- fit_model(new_formula, input)   
        if (!is.null(fit$error)) { # NEW LINE
        # Log error if predictor is bad # NEW LINE
          cat("Error for predictor", predictor, ":", fit$error, "\n") # NEW LINE
          next  # Skip to next iteration if errror # NEW LINE
        } # NEW LINE
        candidate_models[[formula_str]] <- fit$aic # store the new model and AIC in cadidate model list
        print(paste("Testing build-up formula:", formula_str, "with AIC:", fit$aic)) # helpful output
    }
    # ID the best model
    if (length(candidate_models) > 0) {
        best_candidate <- which.min(unlist(candidate_models)) # ID model with lowest AIC
        best_candidate_formula <- names(candidate_models)[best_candidate] # Get formula of best candidate model (string)
        best_candidate_aic <- unlist(candidate_models[best_candidate_formula])
        # Update to the new best model if it improves AIC
        if (best_candidate_aic < best_aic) { # If the AIC of the best candidate model is lower than the current best model's AIC, it replaces the model with the new model
            best_formula <- as.formula(best_candidate_formula) # Converts the model formula string back into a regular formula object
            best_aic <- best_candidate_aic # Updates best_aic to the AIC of the new best model
            print(paste("New best model:", best_candidate_formula, "AIC:", best_aic)) # helpful output
        } else {
            print("No further improvement, stopping build-up.")
            break  
        }
    } else {
        print("No more predictors to test, stopping build-up.")
        break
    }
}

#####
# Final model
# best_model <- multinom(best_formula, data = data, MaxNWts = 5000, trace = FALSE)
# best_model <- lmer(best_formula, data = input)
# best_model <- clmm(best_formula, data = input, link = "probit")
best_model <- polr(best_formula, data = input, Hess=FALSE, method="probit")
# summary(best_model)
############################################
# PAIR-DOWN PHASE
############################################
current_formula <- best_formula  # start with best FORMULA from build-up phase
# Iteratively remove predictors
repeat {
  predictors_in_model <- all.vars(current_formula)[-1]  # get all predictors currently in the best model
  candidate_models <- list() # list to store models and AIC
  # current_model <- lmer(current_formula, data = input)
  # current_model <- clmm(current_formula, data = input, link = "probit")
  current_model <- polr(current_formula, data = input, Hess=FALSE, method="probit")
  # current_aic <- AIC(multinom(current_formula, data = data, MaxNWts = 5000, trace = FALSE)) # calculate AIC of current best model
  current_aic <- AIC(current_model)
  
  for (predictor in predictors_in_model) { # Loop through each predictors to test their removal
    pairdown_formula <- as.formula(paste(response_var, "~", paste(setdiff(predictors_in_model, predictor), collapse = "+"))) # New formula without current predictor
    if (length(all.vars(pairdown_formula)[-1]) == 0) { # check if model is empty (no predictors)
      next  # skip iteration if no predictors are left
    }
    # pairdown_aic <- AIC(multinom(pairdown_formula, data = data, MaxNWts = 5000, trace = FALSE)) # fit pairdown model, get AIC
    fit <- fit_model(pairdown_formula, input)
    formula_str <- paste(deparse(pairdown_formula, width.cutoff = 500), collapse = "") # store the name/string of the model properly
    candidate_models[formula_str] <- fit$aic # Store AIC and formula of pairdown model
    print(paste("Testing pairdown formula:", deparse(pairdown_formula), "with AIC:", pairdown_aic)) # Helpful output
  }
  
  # See if any pairdown model is better than current best model
  if (length(candidate_models) > 0) { 
    best_pairdown_aic <- min(sapply(candidate_models, identity)) # find smallest AIC among pairdown models
    if (best_pairdown_aic < current_aic) { # If a pairdown model has lower AIC, update current best model
      best_pairdown_formula <- names(candidate_models)[which.min(sapply(candidate_models, identity))]
      current_formula <- as.formula(best_pairdown_formula)
      current_aic <- best_pairdown_aic
      print(paste("New best pairdown model:", best_pairdown_formula, "AIC:", best_pairdown_aic)) # Output new model's formula and AIC
    } else {
      print("No further improvement, final model selected.")
      break  # Exit loop if no improvement
    }
  } else {
    print("No improvement, stopping pair-down phase.") # If no pairdown models found with lower AIC, then stops the process
    break
  }
}
# Final pairdown model
# final_model <- multinom(current_formula, data = data, MaxNWts = 5000, trace = FALSE)
# final_model <- lmer(current_formula, data = input)
# final_model <- clmm(current_formula, data = input, link = "probit")
final_model <- polr(current_formula, data = input, Hess=FALSE, method="probit")
# summary(final_model)
# final formula
final_form <- formula(final_model)
# print(final_form)
# # Useful info for meta-analysis
coeff <- coef(final_model) # grab coefficients
# standard_err <- sqrt(diag(vcov(final_model))) # Calculate SE (method OK?)
# confidence_intervals <- confint(final_model, level = 0.95) # Calculate CI
odds_ratios <- exp(coeff) # Calculate OR
# `assign` to new variables based on name prefix chosen
assign(paste0(name_prefix, "_final_model"), final_model)
assign(paste0(name_prefix, "_final_form"), final_form)
assign(paste0(name_prefix, "_odds_ratios"), odds_ratios)
# save for later
output_directory <- "output" # output directory
saveRDS(get(paste0(name_prefix, "_final_model")), file = file.path(output_directory, paste0(name_prefix, "_final_model.rds")))
saveRDS(get(paste0(name_prefix, "_final_form")), file = file.path(output_directory, paste0(name_prefix, "_final_form.rds")))
saveRDS(get(paste0(name_prefix, "_odds_ratios")), file = file.path(output_directory, paste0(name_prefix, "_odds_ratios.rds")))

```

### Parallel (Multi-Core) Version
```{R, echo=FALSE}
placeholder
```

### Output
Output of the model selection script can be found in the `output` folder.