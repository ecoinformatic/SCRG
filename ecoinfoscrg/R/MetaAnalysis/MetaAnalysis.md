# Meta-Analysis Workflow

## Wrangling and Cleaning Data (`wranglingCleaning.R`)
The following script imports datasets generated by the following LSSMs:
* [Choctawhatchee Bay Living Shoreline Model (VIMS)](https://www.arcgis.com/home/webmap/viewer.html?webmap=9b2ef272b1af429fa38e67134ac49da3)
* [Pensacola Bay Living Shoreline Model (VIMS)](https://www.arcgis.com/apps/instant/interactivelegend/index.html?appid=e9eee63681694840a5a7d8ee41f19eb6)
* [Living Shoreline Suitability Model for Tampa Bay (VIMS)](https://docs.google.com/spreadsheets/d/1jZmUNlY68Eb_SUPPAjriNE0UJUHFtkh_xXRLeNGBI_g/edit?gid=0#gid=0)
* [Shoreline Restoration Suitability Model for North Indian River and Mosquito Lagoon](https://ucfonline.maps.arcgis.com/apps/MapSeries/index.html?appid=45caa29e80e6441c8bf6f75c542860af)

In this script, certain types of columns are removed to retain only relevant predictor and response variables. Columns containing non-predictor metadata (e.g., those associated with the date of study or link to materials) and columns containing geometry data (e.g., those needed for GIS shapefiles but not for model selection) are removed. Additionally, spelling checks are perfomed to ensure column naming is consistent across datasets (e.g., replacing 'widebeach' with 'WideBeach'), and that values within columns are correctly spelled and formated (e.g., replacing "YEs" with "Yes").

```{R, echo=FALSE}
rm(list=ls())
library(metafor)
library(sf)
library(dplyr)

# Choctawatchee data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
choc <- st_transform(st_read("/home/gzaragosa/data/choctawatchee_bay_lssm/choctawatchee_bay_lssm_POINTS_0.001deg.shp"), crs = 6346) # Reponse: SMMv5Class

# Pensacola data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
pens <- st_transform(st_read("/home/gzaragosa/data/pensacola_lssm/Santa_Rosa_Bay_Living_Shoreline_POINTS_0.001deg.shp"), crs = 6346) # Reponse: SMMv5Class

# Tampa data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
tampa <- st_transform(st_read("/home/gzaragosa/data/Tampa_Bay_Living_Shoreline_Suitability_Model_Results/Tampa_Bay_Living_Shoreline_Suitability_Model_Results_POINTS_0.001deg.shp"), crs = 6346) # Response: 

# IRL data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
IRL <- st_transform(st_read("/home/gzaragosa/data/Final NIRL Shapefile_all data/Final Shapefile_all data/UCF_livingshorelinemodels_MosquitoNorthIRL_111m.shp"), crs = 6346)

# colnames(choc)
# colnames(pens)
# colnames(tampa)
# colnames(IRL)
# View(choc)
# View(pens)
# View(tampa)
# View(IRL)

####################################
# RESPONSE VARIABLES
####################################
# choc$SMMv5Class, pens$SMMv5Class, tampa$BMPallSMM # Reponse variables for VIMS
# IRL$Priority $ Response variable for IRL study

####################################
# PREDICTOR DATA
####################################
# Columns that are geodata/metadata and can be removed across studies
choc <- st_drop_geometry(choc)
pens <- st_drop_geometry(pens)
tampa <- st_drop_geometry(tampa)
IRL <- st_drop_geometry(IRL)
drop <- c("OBJECTID", "ID", "geometry", "feature_x", "feature_y", "nearest_x", "nearest_y", "shape__len", "Shape__Len", "distance", "distance_2", "n", "x", "y", "X", "Y")
choc <- choc[, !(colnames(choc) %in% drop)]
pens <- pens[, !(colnames(pens) %in% drop)]
tampa <- tampa[, !(colnames(tampa) %in% drop)]
IRL <- IRL[, !(colnames(IRL) %in% drop)]

# Columns that can be removed from individual studies 
# Remove metadata columns and other shapefile stuff
choc <- choc[, !(colnames(choc) %in% c("DefDate", "Needs_QC", "bmpCountv5", "SMMv5Def"))] # DefDate is just a date of data entry
pens <- pens[, !(colnames(pens) %in% c("Additional", "Permitting"))] # "Additional" and "Permitting" are links in the pens data
tampa <- tampa[, !(colnames(tampa) %in% c("Source"))]
IRL <- IRL[, !(colnames(IRL) %in% c("Comments"))]

# Rename columns to be consisent
## Columns that should be the same:
### choc$MxQExpCode, pens$exposure, tampa$Exposure
### choc$Beach, pens$beach, tampa$Beach
### choc$WideBeach, pens$widebeach, tampa$WideBeach
choc <- choc %>% rename(
  Exposure = MxQExpCode,
  Beach = Beach,
  WideBeach = WideBeach,
  Response = SMMv5Class
)
pens <- pens %>% rename(
  Exposure = exposure,
  Beach = beach,
  WideBeach = widebeach,
  Response = SMMv5Class
)
tampa <- tampa %>% rename(
  Exposure = Exposure,
  Beach = Beach,
  WideBeach = WideBeach,
  Response = BMPallSMM
)
IRL$Priority <- as.character(IRL$Priority)
IRL <- IRL %>% rename(
  Response = Priority
)

# Add a 'study' column
choc$study <- 'choc'
pens$study <- 'pens'
tampa$study <- 'tampa'
IRL$study <- 'IRL'

# Combine Data
state <- dplyr::bind_rows(choc, pens, tampa, IRL) 
pred <- state %>% 
  select(-"Response") # Remove response variables

####################################
# SPELL CHECK
####################################
library(hunspell)
library(dplyr)
library(stringr)

# Review misspelled/suggestions
words_to_check <- unique(unlist(pred))
misspelled_info <- hunspell_check(words_to_check)
misspelled_words <- words_to_check[!misspelled_info]
suggestions <- hunspell_suggest(misspelled_words)
print(data.frame(misspelled = misspelled_words, suggestions = I(suggestions)))

# Spelling corrections (words needs to be chosen manually)
corrections <- data.frame(
  incorrect = c("YEs", "RIprap", "riprap", "Permament", "Permanenet", "Bulkead", "Bulkhea", "BUlkhead"),
  correct = c("Yes", "Riprap", "Riprap", "Permanent", "Permanent", "Bulkhead", "Bulkhead", "Bulkhead")
)

pred <- pred %>%
  mutate(across(where(is.character), ~{
    column <- .
    for (i in seq_along(corrections$incorrect)) {
      # Use regex to match case-insensitively
      pattern <- str_c("(?i)\\b", corrections$incorrect[i], "\\b")
      column <- str_replace_all(column, regex(pattern), corrections$correct[i])
    }
    column
  }))


# ################### UTILS/UNIQUE VALUES ####################
# # Get observed states for each column:
# table(tolower(pens$exposure), useNA = "ifany") # Count info
# table(tolower(choc$MxQExpCode), useNA = "ifany")
# table(tolower(tampa$Exposure), useNA = "ifany")
# table(tolower(pens$exposure), useNA = "ifany")
# ############################################################
```

## A.1/A.2: Standardizing Predictors (`standardize.R`)

### Numerical Variables
To ensure models run without errors, missing values (NA) must be replaced. For numerical variables, we can replace the missing values with the statewide mean (calculated as the mean across all regional datasets). They can then be standardized across studies for consistency.

```{R, echo=FALSE}
library(dplyr)
library(tidyr)
# View(pred)

#########################
# NUMERICAL VARS
#########################
# List numerical vars
numerical_vars <- c("angle", "SL_Length", "SL_Lgth_mi", "IT_Width", "Hab_W1", 
                    "Hab_W2", "Hab_W3", "Hab_W4", "Slope", "X3_m_depth", "X5_m_depth", "Slope_4",
                     "X10th", "X20th", "X30th", "X40th", "X50th", "X60th", "X70th", "X80th",
                     "X90th", "X99th", "Length", "MANGROVE")
pred <- pred %>% 
    mutate(across(all_of(numerical_vars), as.numeric)) # convert them to numeric if not already

# Replace NAs with means
pred <- pred %>%
  mutate(across(all_of(numerical_vars), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Standardize numeric vars
pred <- pred %>%
  mutate(across(all_of(numerical_vars), ~ (.-mean(., na.rm = TRUE))/sd(., na.rm = TRUE)))

# Checks
## make sure sd is close to 1 and mean is close to 0
summary(pred[numerical_vars]) # summary stats
sapply(pred[numerical_vars], sd, na.rm = TRUE) # stdev
```

### Categorical Variables
The following chunk also tranforms each possible outcome of a categorical predictor to a "dummy variable" column, with a '1' indicating an observation of that predictor value. For missing values, a "_Missing" column is added which can be included or excluded from downstream analyses.

```{R, echo=FALSE}
#########################
# CATEGORICAL VARS
#########################
# List cat vars
categorical_vars <- c("bnk_height", "Beach", "WideBeach", "Exposure", "bathymetry", 
                      "roads", "PermStruc", "PublicRamp", "RiparianLU", "canal", 
                      "SandSpit", "Structure", "offshorest", "SAV", "marsh_all", 
                      "tribs", "defended", "rd_pstruc", "lowBnkStrc", "ShlType", 
                      "Fetch_", "selectThis", "StrucList", "forestshl", 
                      "City", "Point_Type", "Edge_Type", "Hard_Mater", "Adj_LU", 
                      "Erosion", "Erosion_2", "Owner", "Adj_H1", "Adj_H2", "Adj_H3", "Adj_H4", "V_Type1", 
                      "V_Type2", "V_Type3", "V_Type4", "Rest_Opp", "X0yster_Pre", "Seagrass_P",
                      "Hardened_1", "WTLD_VEG_3") 
                      # note that study column is excluded here for easier processing later
pred <- pred %>%
    mutate(across(all_of(categorical_vars), as.factor))

##### DUMMY VARIABLES #####
# Replace NA with "Missing" for dummy vars
pred <- pred %>%
  mutate(across(all_of(categorical_vars), ~ factor(ifelse(is.na(.), "Missing", .), levels = unique(c(.,"Missing")))))

# Make dummy vars
for (var in categorical_vars) {
  dummies <- model.matrix(~ . - 1, data = pred[var])  # suggested to avoid itercept
  colnames(dummies) <- paste(var, levels(pred[[var]]), sep = "_")
  pred <- cbind(pred, as.data.frame(dummies))
}

# Remove OG categorical columns
pred <- pred %>%
  select(-all_of(setdiff(categorical_vars, "study")))

# Check data
str(pred)
```

## A.3/A.4: Pruning & Model Selection (`pruning.R`)
The following script is used to prepare predictor and response variables for the model selection workflow contained in `BUPD.R`. The script generates the following outputs, though additional outputs can be added as necessary:
* `"_final_model.rds"`: Contains model summary and slopes for predictors
* `"_final_form.rds"`: Contains final model formula with best AIC
* `"_odds_ratios.rds"`: Contains odds ratios for predictors

```{R, echo=FALSE}
#############################################
# GET RESPONSE AND STANDARDIZED PREDICTORS
#############################################
library(dplyr)
resp <- as.data.frame(cbind(state$Response, state$study))
colnames(resp) <- c("Response", "study")
# str(resp)
# str(pred)
# View(resp)
# View(pred)

resp_choc <- resp %>% filter(study == "choc")
resp_pens <- resp %>% filter(study == "pens")
resp_tampa <- resp %>% filter(study == "tampa")
resp_IRL <- resp %>% filter(study == "IRL")

pred_choc <- pred %>% filter(study == "choc")
pred_pens <- pred %>% filter(study == "pens")
pred_tampa <- pred %>% filter(study == "tampa")
pred_IRL <- pred %>% filter(study == "IRL")

##### CHOSE STUDY HERE #####
# combine response and pred
data <- cbind(resp_IRL, pred_IRL) # choc example

# Specify a short name of the model
name <- "placeholderName"
############################

# Grab categorical variables (dummyvars has the separated out names/dummy variables)
dummyvars <- colnames(pred)[grepl("_", colnames(pred))]

# List predictors (AKA column names of known predictors)
predictors <- c(numerical_vars, dummyvars)

# Define the response variable
response_var <- "Response" 
# response_var <- "BMPallSMM" 

# # Run build-up/pair-down R script
start_time <- Sys.time()
source("MetaAnalysis/BUPD.R")
end_time <- Sys.time()

# output_formula <- readRDS("/home/gzaragosa/Documents/SCRG/MetaAnalysis/Routput/chocTest_final_form.rds")
# output_model <- readRDS("/home/gzaragosa/Documents/SCRG/MetaAnalysis/Routput/chocTest_final_model.rds")
# output_OR <- readRDS("/home/gzaragosa/Documents/SCRG/MetaAnalysis/Routput/chocTest_odds_ratios.rds")
```

### Build-Up, Pair-Down Model Selection Script (`BUPD`)
The following is the build-up, pair-down script used to perform model selection:

```{R, echo=FALSE}
############################################
# INITIALIZE
############################################
library(nnet)
# Initial empty model
# Note that `best_formula` starts with `initial_formula` as baseline
initial_formula <- as.formula(paste(response_var, "~ 1"))
best_formula <- initial_formula
best_model <- multinom(best_formula, data = data, MaxNWts = 5000)
best_aic <- AIC(best_model)
name_prefix <- gsub(" ", "_", name) # add underscores
############################################
# BUILD-UP PHASE
############################################
# Function to fit and evaluate models (similar to Chris' only with a multinomial logistic regression for this specific dataset)
fit_and_evaluate <- function(formula, data) {
  model <- multinom(formula, data = data, MaxNWts = 5000, trace = FALSE) # Note: Using multinomial logistic regression
  AIC(model)  # Using AIC for simplicity, but you can choose other criteria
}
# Similar to Chris' only with a multinomial logistic regression (which can be switched out)
for (i in 1:length(predictors)) {
    current_predictors <- all.vars(best_formula)[-1]
    remaining_predictors <- setdiff(predictors, current_predictors) 
    candidate_models <- list()  # for storing model and their AIC
    # Build models iteratively based on current best model
    for (predictor in remaining_predictors) {
        new_formula <- update(best_formula, paste(". ~ . +", predictor))
        formula_str <- paste(deparse(new_formula, width.cutoff = 500), collapse = "") # converts model formulat to string. Note that the width.cutoff and collapse are EXTREMELY important or deparse will split your string into two lines by default
        candidate_aic <- fit_and_evaluate(new_formula, data) # using the `fit_and_evaluate` function from above to fit new model and calculate AIC
        candidate_models[[formula_str]] <- candidate_aic # store the new model and AIC in cadidate model list
        print(paste("Testing build-up formula:", formula_str, "with AIC:", candidate_aic)) # helpful output
    }
    # ID the best model
    if (length(candidate_models) > 0) {
        best_candidate <- which.min(unlist(candidate_models)) # ID model with lowest AIC
        best_candidate_formula <- names(candidate_models)[best_candidate] # Get formula of best candidate model (string)
        best_candidate_aic <- unlist(candidate_models[best_candidate_formula])
        # Update to the new best model if it improves AIC
        if (best_candidate_aic < best_aic) { # If the AIC of the best candidate model is lower than the current best model's AIC, it replaces the model with the new model
            best_formula <- as.formula(best_candidate_formula) # Converts the model formula string back into a regular formula object
            best_aic <- best_candidate_aic # Updates best_aic to the AIC of the new best model
            print(paste("New best model:", best_candidate_formula, "AIC:", best_aic)) # helpful output
        } else {
            print("No further improvement, stopping build-up.")
            break  
        }
    } else {
        print("No more predictors to test, stopping build-up.")
        break
    }
}
#####
# Final model
best_model <- multinom(best_formula, data = data, MaxNWts = 5000, trace = FALSE)
# summary(best_model)

############################################
# PAIR-DOWN PHASE
############################################
current_formula <- best_formula  # start with best FORMULA from build-up phase
# Iteratively remove predictors
repeat {
  predictors_in_model <- all.vars(current_formula)[-1]  # get all predictors currently in the best model
  candidate_models <- list() # list to store models and AIC
  current_aic <- AIC(multinom(current_formula, data = data, MaxNWts = 5000, trace = FALSE)) # calculate AIC of current best model
  
  for (predictor in predictors_in_model) { # Loop through each predictors to test their removal
    pairdown_formula <- as.formula(paste(response_var, "~", paste(setdiff(predictors_in_model, predictor), collapse = "+"))) # New formula without current predictor
    if (length(all.vars(pairdown_formula)[-1]) == 0) { # check if model is empty (no predictors)
      next  # skip iteration if no predictors are left
    }
    pairdown_aic <- AIC(multinom(pairdown_formula, data = data, MaxNWts = 5000, trace = FALSE)) # fit pairdown model, get AIC
    formula_str <- paste(deparse(pairdown_formula, width.cutoff = 500), collapse = "") # store the name/string of the model properly
    candidate_models[formula_str] <- pairdown_aic # Store AIC and formula of pairdown model
    print(paste("Testing pairdown formula:", deparse(pairdown_formula), "with AIC:", pairdown_aic)) # Helpful output
  }
  
  # See if any pairdown model is better than current best model
  if (length(candidate_models) > 0) { 
    best_pairdown_aic <- min(sapply(candidate_models, identity)) # find smallest AIC among pairdown models
    if (best_pairdown_aic < current_aic) { # If a pairdown model has lower AIC, update current best model
      best_pairdown_formula <- names(candidate_models)[which.min(sapply(candidate_models, identity))]
      current_formula <- as.formula(best_pairdown_formula)
      current_aic <- best_pairdown_aic
      print(paste("New best pairdown model:", best_pairdown_formula, "AIC:", best_pairdown_aic)) # Output new model's formula and AIC
    } else {
      print("No further improvement, final model selected.")
      break  # Exit loop if no improvement
    }
  } else {
    print("No improvement, stopping pair-down phase.") # If no pairdown models found with lower AIC, then stops the process
    break
  }
}
# Final pairdown model
final_model <- multinom(current_formula, data = data, MaxNWts = 5000, trace = FALSE)
# summary(final_model)
# final formula
final_form <- formula(final_model)
# print(final_form)
# # Useful info for meta-analysis
coeff <- coef(final_model) # grab coefficients
# standard_err <- sqrt(diag(vcov(final_model))) # Calculate SE (method OK?)
# confidence_intervals <- confint(final_model, level = 0.95) # Calculate CI
odds_ratios <- exp(coeff) # Calculate OR
# `assign` to new variables based on name prefix chosen
assign(paste0(name_prefix, "_final_model"), final_model)
assign(paste0(name_prefix, "_final_form"), final_form)
assign(paste0(name_prefix, "_odds_ratios"), odds_ratios)
# save for later
output_directory <- "/home/gzaragosa/Documents/SCRG/MetaAnalysis/Routput"
saveRDS(get(paste0(name_prefix, "_final_model")), file = file.path(output_directory, paste0(name_prefix, "_final_model.rds")))
saveRDS(get(paste0(name_prefix, "_final_form")), file = file.path(output_directory, paste0(name_prefix, "_final_form.rds")))
saveRDS(get(paste0(name_prefix, "_odds_ratios")), file = file.path(output_directory, paste0(name_prefix, "_odds_ratios.rds")))
```

### Output
Output of the model selection script can be found in the `SCRG/MetaAnalysis/Routput` folder.

## A.5: Constructing Well-Conditioned Uncertainty Matrices
For each local study, missing parameter estimates are replaced with sample means. Missing covariances in the covariance matrix of parameters are set to 0, and missing variances are set to large values. This aids in maintaining matrix stability.

### Retrieve Betas from Model Output (`getBetas.R`)
The following script retrieves the parameter estimates generated by the `BUPD.R` script and creates a dataset with those betas listed in column which their respective response variables in rows. There are two primary outputs:
* `combined_betas`: contains parameter estimates and columns containin info regarding the model, study, and Intercept.
* `combined_betas_only`: contains only parameter estimates (model, study, and Intercept are removed)

```{R, echo=FALSE}
library(nnet)
library(dplyr)

# Get models from .rds
model_paths <- c("MetaAnalysis/Routput/chocTest_final_model.rds",
                 "MetaAnalysis/Routput/pensTest_final_model.rds",
                 "MetaAnalysis/Routput/tampaTest_final_model.rds",
                 "MetaAnalysis/Routput/IRLTest_final_model.rds")
model_names <- c("choc", "pens", "tampa", "IRL")

# Load and prep model data
prepare_df <- function(model_path, model_name) {
  model <- readRDS(model_path)
  coef_df <- as.data.frame(coef(model))
  coef_df$response_category <- rownames(coef_df)
  coef_df$model <- model_name
  return(coef_df)
}

# Grab and combine betas
model_data_frames <- Map(prepare_df, model_paths, model_names)

# Create data frames for each model
choc_betas <- model_data_frames[[1]]
pens_betas <- model_data_frames[[2]]
tampa_betas <- model_data_frames[[3]]
IRL_betas <- model_data_frames[[4]]

# Combine all individual data frames
combined_betas <- bind_rows(model_data_frames)
combined_betas <- combined_betas %>%
  arrange(response_category, model) # Rearrange/move response to the left 

# check
choc_betas
pens_betas
tampa_betas
IRL_betas
combined_betas

#######################
# GET COLUMNS FOR EACH STUDY
#######################
# Generate dummy variable names function
generate_dummy_colnames <- function(base_cols, pred_cols) {
  dummy_colnames <- pred_cols[grepl(paste(base_cols, collapse = "|"), pred_cols)]
  return(dummy_colnames)
}

# Extract "base" (original) column names from dummy variables in `pred`
base_pred_cols <- unique(sub("_.*", "", colnames(pred)))

# Match study columns to dummy variable format in pred
match_study_to_pred <- function(study_cols, pred_cols) {
  base_study_cols <- intersect(study_cols, base_pred_cols)
  dummy_colnames <- generate_dummy_colnames(base_study_cols, pred_cols)
  return(dummy_colnames)
}

# Get dummy variable names for studies
choc_dcolnames <- match_study_to_pred(colnames(choc), colnames(pred))
pens_dcolnames <- match_study_to_pred(colnames(pens), colnames(pred))
tampa_dcolnames <- match_study_to_pred(colnames(tampa), colnames(pred))
IRL_dcolnames <- match_study_to_pred(colnames(IRL), colnames(pred))
choc_dcolnames
pens_dcolnames
tampa_dcolnames
IRL_dcolnames

#######################
# ENSURE STUDY BETAS CONTAIN NECESSARY COLUMNS
#######################
# Function to add missing columns as numeric
add_missing_columns <- function(df, colnames_vec) {
  missing_cols <- setdiff(colnames_vec, colnames(df))
  for (col in missing_cols) {
    df[[col]] <- as.numeric(NA)
  }
  return(df)
}

# Add missing columns to each study's betas dataframe
choc_betas <- add_missing_columns(choc_betas, choc_dcolnames)
pens_betas <- add_missing_columns(pens_betas, pens_dcolnames)
tampa_betas <- add_missing_columns(tampa_betas, tampa_dcolnames)
IRL_betas <- add_missing_columns(IRL_betas, IRL_dcolnames)
convert_to_numeric <- function(df) {
  df[] <- lapply(df, function(col) if(is.logical(col)) as.numeric(col) else col)
  return(df)
}

choc_betas <- convert_to_numeric(choc_betas)
pens_betas <- convert_to_numeric(pens_betas)
tampa_betas <- convert_to_numeric(tampa_betas)
IRL_betas <- convert_to_numeric(IRL_betas)

#######################
# CALCULATE AND FILL STUDY AVERAGES
#######################
# Helpful for averaging:
calculate_study_average <- function(df) {
  numeric_cols <- df %>%
    select(-`(Intercept)`, where(is.numeric)) %>%
    select_if(~ is.numeric(.) && !all(is.na(.))) # make sure they're numeric!!!
  avg <- mean(unlist(lapply(numeric_cols, as.numeric)), na.rm = TRUE)
  return(avg)
}

# Calculate study beta averages
choc_avg <- calculate_study_average(choc_betas)
pens_avg <- calculate_study_average(pens_betas)
tampa_avg <- calculate_study_average(tampa_betas)
IRL_avg <- calculate_study_average(IRL_betas)
choc_avg
pens_avg
tampa_avg
IRL_avg

# Fill missing values with the study average
fill_missing_with_average <- function(df, average) {
  numeric_cols <- df %>%
    select(-`(Intercept)`, where(is.numeric))
  df[names(numeric_cols)] <- lapply(numeric_cols, function(col) {
    ifelse(is.na(col), average, col)
  })
  return(df)
}

# Relace missing values with averages
choc_betas <- fill_missing_with_average(choc_betas, choc_avg)
pens_betas <- fill_missing_with_average(pens_betas, pens_avg)
tampa_betas <- fill_missing_with_average(tampa_betas, tampa_avg)
IRL_betas <- fill_missing_with_average(IRL_betas, IRL_avg)

# New column for "response_category"
choc_betas$response_category <- rownames(choc_betas)
pens_betas$response_category <- rownames(pens_betas)
tampa_betas$response_category <- rownames(tampa_betas)
IRL_betas$response_category <- rownames(IRL_betas)

# Remove row names
rownames(choc_betas) <- NULL
rownames(pens_betas) <- NULL
rownames(tampa_betas) <- NULL
rownames(IRL_betas) <- NULL

print(head(choc_betas))
print(head(pens_betas))
print(head(tampa_betas))
print(head(IRL_betas))

######### YOU ARE HERE #########
# Combine datasets
combined_betas <- bind_rows(choc_betas, pens_betas, tampa_betas, IRL_betas)

# Ignoring character columns, study column, (Intercept) re and other character columns and intercept
combined_betas_only <- combined_betas %>%
  select(-response_category, -model, -study, -`(Intercept)`, where(is.numeric))
View(combined_betas_only)

## Note: Both "NA" and "Invalid Number" are present
```

### Generate Variance-Covariance Matrix (`varCov.R`)
The following script using the `cov()` function in R to generate a variance-covaraince matrix for the parameter estimates(betas). It replaces missing betas for a study (that were pruned out during model selection) with the mean value for that study. It also replaces missing covariance values with zeros (i.e., no known relationship between parameters) and missing variance values with an arbritrarily high value (high possible range of effects of that parameter on the outcome).

```{R, echo=FALSE}
# Generate covariance matrix
cov_matrix <- cov(combined_betas_only, use = "pairwise.complete.obs")

# Find where there's missing values
missing_values <- is.na(cov_matrix)

# Set missing off-diagonals to zero
cov_matrix[missing_values & !row(cov_matrix) == col(cov_matrix)] <- 0

# Set missing variances to a very large value
large_value <- 1000000 # arbitrary; might need to adjust later
diag(cov_matrix)[missing_values[diag(TRUE, nrow(cov_matrix))]] <- large_value
```

## A.6: Meta-Analytic Regression
The following script uses the `metafor` package in R to perform meta-analytic regression.
* Add info about matrix solvers

```{R, echo=FALSE}
# TBD
```